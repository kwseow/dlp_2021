{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3-Sequence Classification of Movie Reviews.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"WJmU4pTpb9j_"},"source":["## Activity 3 – Sequence Classification of Movie Reviews\n","\n","In this activity, we will:\n","*\tA simple LSTM for Sequence Classification\n","*\tLSTM for Sequence Classification With Dropout\n","*\tLSTM and CNN for Sequence Classification \n"]},{"cell_type":"markdown","metadata":{"id":"LyP6NJAocWNl"},"source":["### Mount Google Drive"]},{"cell_type":"markdown","metadata":{"id":"g8RmJiTFcZfH"},"source":["Mount your Google Drive on your runtime using an authorization code.\n","\n","Follow the instruction on the screen to authorise Colab accessing your drive. On your Google Drive, you can access you files prepend with “/content/drive/MyDrive/”"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-0GMlQuNccUR","executionInfo":{"status":"ok","timestamp":1616493960430,"user_tz":-480,"elapsed":708325,"user":{"displayName":"Jimmy Goh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLSkpAlGxDPhWOsoJbyGqn037GeHQO859TaFZZ=s64","userId":"14200143294426892711"}},"outputId":"03c8b905-acd3-4a3c-9100-68fb3d95b6c6"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IjtNKub6cHUu"},"source":["### Problem Description"]},{"cell_type":"markdown","metadata":{"id":"RbXCd6C5cLG9"},"source":["The problem that we will use to demonstrate sequence learning in this tutorial is the IMDB movie review sentiment classification problem.  We can quickly develop a small LSTM for the IMDB problem and achieve good accuracy. Let’s start off by importing the classes and functions required for this model and initializing the random number generator to a constant value to ensure we can easily reproduce the results.\n","\n","The dataset used in this project is the Large Movie Review Dataset often referred to as the IMDB dataset. The IMDB dataset contains 50,000 highly popular movie reviews (good or bad) for training and the same amount again for testing.  The data was collected by Stanford researchers and was used in a 2011 paper where a split of 50-50 of the data was used for training and test. An accuracy of 88.89% was achieved.\n","\n","The **imdb.load_data()** function allows you to load the dataset in a format that is ready for use in neural network and deep learning models. The words have been replaced by integers that indicate the absolute popularity of the word in the dataset. The sentences in each review are therefore comprised of a sequence of integers.\n","\n","Calling imdb.load_data() the first time will download the IMDB dataset to your computer and store it in your home directory under ~/.keras/datasets/imdb.pkl as a 32 megabyte file. Usefully, the imdb.load_data() function provides additional arguments including the number of top words to load (where words with a lower integer are marked as zero in the returned data), the number of top words to skip (to avoid the the’s) and the maximum length of reviews to support. Let’s load the dataset and calculate some properties of it. We will start off by loading some libraries and loading the entire IMDB dataset as a training dataset\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0rxq8qd7bqD6","executionInfo":{"status":"ok","timestamp":1616497109284,"user_tz":-480,"elapsed":5377,"user":{"displayName":"Jimmy Goh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLSkpAlGxDPhWOsoJbyGqn037GeHQO859TaFZZ=s64","userId":"14200143294426892711"}},"outputId":"40fdd910-bb88-4d7c-ee2d-77c01ddafb4e"},"source":["import numpy\n","from keras.datasets import imdb\n","from matplotlib import pyplot\n","# load the dataset\n","(X_train, y_train), (X_test, y_test) = imdb.load_data()\n","X = numpy.concatenate((X_train, X_test), axis=0)\n","y = numpy.concatenate((y_train, y_test), axis=0)"],"execution_count":29,"outputs":[{"output_type":"stream","text":["<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"CNnbUAlncsew"},"source":["We can display the shape of the training dataset."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2_E0eTUBcxT-","executionInfo":{"status":"ok","timestamp":1616497112474,"user_tz":-480,"elapsed":823,"user":{"displayName":"Jimmy Goh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLSkpAlGxDPhWOsoJbyGqn037GeHQO859TaFZZ=s64","userId":"14200143294426892711"}},"outputId":"f59a2f0d-9910-4ff0-97d9-90c217680098"},"source":["# summarize size\n","print(\"Training data: \")\n","print(X.shape)\n","print(y.shape)"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Training data: \n","(50000,)\n","(50000,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VsKIxVcDcy_o"},"source":["We can also print the unique class values."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZK75lyAqc2vi","executionInfo":{"status":"ok","timestamp":1616497114936,"user_tz":-480,"elapsed":877,"user":{"displayName":"Jimmy Goh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLSkpAlGxDPhWOsoJbyGqn037GeHQO859TaFZZ=s64","userId":"14200143294426892711"}},"outputId":"e46d4d9b-6dcd-4b89-fce3-01fa64462933"},"source":["# Summarize number of classes\n","print(\"Classes: \")\n","print(numpy.unique(y))"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Classes: \n","[0 1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"egiCtfOqc6w5"},"source":["### A simple LSTM for Sequence Classification"]},{"cell_type":"markdown","metadata":{"id":"FQq1oHP9c8dJ"},"source":["Let’s start off by importing the classes and functions required for this model and initializing the random number generator to a constant value to ensure we can easily reproduce the results."]},{"cell_type":"code","metadata":{"id":"K8O16s3mc_va","executionInfo":{"status":"ok","timestamp":1616493966461,"user_tz":-480,"elapsed":714327,"user":{"displayName":"Jimmy Goh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLSkpAlGxDPhWOsoJbyGqn037GeHQO859TaFZZ=s64","userId":"14200143294426892711"}}},"source":["# LSTM for sequence classification in the IMDB dataset\n","import numpy\n","from keras.datasets import imdb\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing import sequence\n","\n","# fix random seed for reproducibility\n","numpy.random.seed(7)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PAlx1GXwdGuQ"},"source":["b)\tWe need to load the IMDB dataset. We are constraining the dataset to the top 5,000 words. We also split the dataset into train (50%) and test (50%) sets."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MpKylhFqdI_G","executionInfo":{"status":"ok","timestamp":1616493971362,"user_tz":-480,"elapsed":719220,"user":{"displayName":"Jimmy Goh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLSkpAlGxDPhWOsoJbyGqn037GeHQO859TaFZZ=s64","userId":"14200143294426892711"}},"outputId":"b480ae13-722a-4ce5-afd7-658152d06e0e"},"source":["# load the dataset but only keep the top n words, zero the rest\n","top_words = 5000\n","(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"cRnewbtIdMuB"},"source":["Next, we need to truncate and pad the input sequences so that they are all the same length for modelling. The model will learn the zero values carry no information so indeed the sequences are not the same length in terms of content, but same length vectors is required to perform the computation in Keras."]},{"cell_type":"code","metadata":{"id":"Ku1Q60uVdQg-","executionInfo":{"status":"ok","timestamp":1616493972470,"user_tz":-480,"elapsed":720326,"user":{"displayName":"Jimmy Goh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLSkpAlGxDPhWOsoJbyGqn037GeHQO859TaFZZ=s64","userId":"14200143294426892711"}}},"source":["# truncate and pad input sequences\n","max_review_length = 500\n","X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n","X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wY8Q-ZNfdaHv"},"source":["We can now define, compile and fit our LSTM model. The first layer is the Embedded layer * that uses 32 length vectors to represent each word. The next layer is the LSTM layer with 100 memory units (smart neurons). Finally, because this is a classification problem we use a Dense output layer with a single neuron and a sigmoid activation function to make 0 or 1 predictions for the two classes (good and bad) in the problem. Because it is a binary classification problem, log loss is used as the loss function (binary crossentropy in Keras). The efficient ADAM optimization algorithm is used. The model is fit for only 3 epochs because it quickly overfits the problem. A large batch size of 64 reviews is used to space out weight updates."]},{"cell_type":"markdown","metadata":{"id":"pwXWL5LMdbtT"},"source":["A breakthrough in the field of natural language processing is called ***word embedding***. This is a technique where words are encoded as real-valued vectors in a high dimensional space, where the similarity between words in terms of meaning translates to closeness in the vector space.  Discrete words are mapped to vectors of continuous numbers. This is useful when working with natural language problems with neural networks as we require numbers as input values.  Keras provides a convenient way to convert positive integer representations of words into a word embedding by an Embedding layer. The layer takes arguments that define the mapping including the maximum number of expected words also called the vocabulary size (e.g. the largest integer value that will be seen as an input). The layer also allows you to specify the dimensionality for each word vector, called the output dimension. We would like to use a word embedding representation for the IMDB dataset. Let us say that we are only interested in the first 5,000 most used words in the dataset. Therefore, our vocabulary size will be 5,000. We can choose to use a 32-dimensional vector to represent each word. Finally, we may choose to cap the maximum review length at 500 words, truncating reviews longer than that and padding reviews shorter than that with 0 values."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"erKdP-9EdfTD","executionInfo":{"status":"ok","timestamp":1616494008542,"user_tz":-480,"elapsed":756391,"user":{"displayName":"Jimmy Goh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLSkpAlGxDPhWOsoJbyGqn037GeHQO859TaFZZ=s64","userId":"14200143294426892711"}},"outputId":"9853f90a-adbb-450f-9f20-617c8685bcfa"},"source":["# create the model\n","embedding_vecor_length = 32\n","model = Sequential()\n","model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n","model.add(LSTM(100))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())\n","model.fit(X_train, y_train, epochs=3, batch_size=64)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 500, 32)           160000    \n","_________________________________________________________________\n","lstm (LSTM)                  (None, 100)               53200     \n","_________________________________________________________________\n","dense (Dense)                (None, 1)                 101       \n","=================================================================\n","Total params: 213,301\n","Trainable params: 213,301\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/3\n","391/391 [==============================] - 14s 27ms/step - loss: 0.5877 - accuracy: 0.6613\n","Epoch 2/3\n","391/391 [==============================] - 11s 27ms/step - loss: 0.3250 - accuracy: 0.8652\n","Epoch 3/3\n","391/391 [==============================] - 11s 27ms/step - loss: 0.2318 - accuracy: 0.9109\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f04a8352a10>"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"KIuyIV9kdqnn"},"source":["Once fit, we estimate the performance of the model on unseen reviews"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qmi_bZX-dr1f","executionInfo":{"status":"ok","timestamp":1616494014769,"user_tz":-480,"elapsed":762611,"user":{"displayName":"Jimmy Goh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLSkpAlGxDPhWOsoJbyGqn037GeHQO859TaFZZ=s64","userId":"14200143294426892711"}},"outputId":"06910e3e-2074-4c6f-e1df-baff1fd4d736"},"source":["# Final evaluation of the model\n","scores = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Accuracy: 77.44%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"imnaepxaepxY"},"source":["You can see that this simple LSTM with little tuning achieves near state-of-the-art results on the IMDB problem. Importantly, this is a template that you can use to apply LSTM networks to your own sequence classification problems."]},{"cell_type":"markdown","metadata":{"id":"7hLCsB7ZfaKD"},"source":["### LSTM for Sequence Classification With Dropout"]},{"cell_type":"markdown","metadata":{"id":"3bxHTjvgfde2"},"source":["Recurrent Neural networks like LSTM generally have the problem of overfitting. Dropout can be applied between layers using the Dropout Keras layer.\n","\n","We can do this easily by adding new Dropout layers between the Embedding and LSTM layers and the LSTM and Dense output layers. For example:\n","\n","\n","\n","```\n","# create the model\n","model = Sequential()\n","model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n","model.add(Dropout(0.2))\n","model.add(LSTM(100))\n","model.add(Dropout(0.2))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1Kuoguwmf0Cm"},"source":["Let's restart from reading in the dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ENhUFb0bfbVU","executionInfo":{"status":"ok","timestamp":1616494020849,"user_tz":-480,"elapsed":768683,"user":{"displayName":"Jimmy Goh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLSkpAlGxDPhWOsoJbyGqn037GeHQO859TaFZZ=s64","userId":"14200143294426892711"}},"outputId":"53ad2342-a052-4376-e500-3f1a03dabd01"},"source":["# LSTM for sequence classification in the IMDB dataset\n","import numpy\n","from keras.datasets import imdb\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Dropout\n","from keras.layers.embeddings import Embedding\n","from keras.preprocessing import sequence\n","\n","# fix random seed for reproducibility\n","numpy.random.seed(7)\n","\n","# load the dataset but only keep the top n words, zero the rest\n","top_words = 5000\n","(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n","\n","# truncate and pad input sequences\n","max_review_length = 500\n","X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n","X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n","\n","embedding_vecor_length = 32"],"execution_count":10,"outputs":[{"output_type":"stream","text":["<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z_4N3pzjf4xT","executionInfo":{"status":"ok","timestamp":1616494020850,"user_tz":-480,"elapsed":768677,"user":{"displayName":"Jimmy Goh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLSkpAlGxDPhWOsoJbyGqn037GeHQO859TaFZZ=s64","userId":"14200143294426892711"}},"outputId":"59be3da6-5b1d-41b0-a090-1451b7a28cd0"},"source":["# create the model\n","model = Sequential()\n","model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n","model.add(Dropout(0.2))\n","model.add(LSTM(100))\n","model.add(Dropout(0.2))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_1 (Embedding)      (None, 500, 32)           160000    \n","_________________________________________________________________\n","dropout (Dropout)            (None, 500, 32)           0         \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 100)               53200     \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 100)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 101       \n","=================================================================\n","Total params: 213,301\n","Trainable params: 213,301\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NhimKpypf7RC","executionInfo":{"status":"ok","timestamp":1616494054681,"user_tz":-480,"elapsed":802501,"user":{"displayName":"Jimmy Goh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLSkpAlGxDPhWOsoJbyGqn037GeHQO859TaFZZ=s64","userId":"14200143294426892711"}},"outputId":"b4500435-5975-4b9c-b743-1fb0ee30f064"},"source":["model.fit(X_train, y_train, epochs=3, batch_size=64)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Epoch 1/3\n","391/391 [==============================] - 12s 28ms/step - loss: 0.5908 - accuracy: 0.6802\n","Epoch 2/3\n","391/391 [==============================] - 11s 28ms/step - loss: 0.3282 - accuracy: 0.8640\n","Epoch 3/3\n","391/391 [==============================] - 11s 27ms/step - loss: 0.2706 - accuracy: 0.8924\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f04af5ba110>"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5q44VVa4f-5F","executionInfo":{"status":"ok","timestamp":1616494060728,"user_tz":-480,"elapsed":808541,"user":{"displayName":"Jimmy Goh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLSkpAlGxDPhWOsoJbyGqn037GeHQO859TaFZZ=s64","userId":"14200143294426892711"}},"outputId":"a3c1678f-b223-4f59-bb81-1cda4fcfe3aa"},"source":["# Final evaluation of the model\n","scores = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Accuracy: 84.99%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"I45x6zNGgCgY"},"source":["We can see dropout having the desired impact on training with a slightly slower trend in convergence and in this case a lower final accuracy. The model could probably use a few more epochs of training and may achieve a higher skill (try it and see)"]},{"cell_type":"markdown","metadata":{"id":"PQag6PxIgRUk"},"source":["Alternately, dropout can be applied to the input and recurrent connections of the memory units with the LSTM precisely and separately. Keras provides this capability with parameters on the LSTM layer, the dropout for configuring the input dropout and recurrent dropout for configuring the recurrent dropout. For example, we can modify the first example to add dropout to the input and recurrent connections as follows:\n","\n","```\n","# create the model\n","model = Sequential()\n","model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n","model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2)) \n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","```\n","Let's try out this model.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cHaxRckLguVY","executionInfo":{"status":"ok","timestamp":1616494060729,"user_tz":-480,"elapsed":808535,"user":{"displayName":"Jimmy Goh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLSkpAlGxDPhWOsoJbyGqn037GeHQO859TaFZZ=s64","userId":"14200143294426892711"}},"outputId":"b1acfad1-dee1-49f1-96b9-1e15e538a9aa"},"source":["# create the model\n","model = Sequential()\n","model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n","model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2)) \n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())"],"execution_count":14,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (None, 500, 32)           160000    \n","_________________________________________________________________\n","lstm_2 (LSTM)                (None, 100)               53200     \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1)                 101       \n","=================================================================\n","Total params: 213,301\n","Trainable params: 213,301\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AlV0owdOgvuN","executionInfo":{"status":"ok","timestamp":1616495914092,"user_tz":-480,"elapsed":2661889,"user":{"displayName":"Jimmy Goh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLSkpAlGxDPhWOsoJbyGqn037GeHQO859TaFZZ=s64","userId":"14200143294426892711"}},"outputId":"93d3398f-3494-4deb-c739-6d36ab579f7b"},"source":["%%time\n","model.fit(X_train, y_train, epochs=3, batch_size=64)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Epoch 1/3\n","391/391 [==============================] - 618s 2s/step - loss: 0.5838 - accuracy: 0.6718\n","Epoch 2/3\n","391/391 [==============================] - 615s 2s/step - loss: 0.3042 - accuracy: 0.8751\n","Epoch 3/3\n","391/391 [==============================] - 620s 2s/step - loss: 0.3079 - accuracy: 0.8762\n","CPU times: user 45min 56s, sys: 6min 9s, total: 52min 6s\n","Wall time: 30min 53s\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f04adb78f50>"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cu3lN8ltg1Vx","executionInfo":{"status":"ok","timestamp":1616495992843,"user_tz":-480,"elapsed":2740633,"user":{"displayName":"Jimmy Goh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLSkpAlGxDPhWOsoJbyGqn037GeHQO859TaFZZ=s64","userId":"14200143294426892711"}},"outputId":"0a340b48-4d2f-421b-87a7-17ca04bf9504"},"source":["# Final evaluation of the model\n","scores = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Accuracy: 87.78%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3L3rw7ERga7V"},"source":["We can see that the LSTM specific dropout has a more pronounced effect on the convergence of the network than the layer-wise dropout. As above, the number of epochs was kept constant and could be increased to see if the skill of the model can be further lifted. Dropout is a powerful technique for combating overfitting in your LSTM models and it is a good idea to try both methods, but you may bet better results with the gate-specific dropout provided in Keras."]},{"cell_type":"markdown","metadata":{"id":"L5CTVyUGjAgT"},"source":["### LSTM and CNN for Sequence Classification "]},{"cell_type":"markdown","metadata":{"id":"hBYqHYh9jFCp"},"source":["Convolutional neural networks excel at learning the spatial structure in input data. The IMDB review data does have a one-dimensional spatial structure in the sequence of words in reviews and the CNN may be able to pick out invariant features for good and bad sentiment.  This learned spatial features may then be learned as sequences by an LSTM layer.\n","\n","We can easily add a one-dimensional CNN and max pooling layers after the Embedding layer which then feed the consolidated features to the LSTM. We can use a smallish set of 32 features with a small filter length of 3. The pooling layer can use the standard length of 2 to halve the feature map size. For example, we would create the model as follows:\n","\n","\n","```\n","# create the model\n","model = Sequential()\n","model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n","model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')) \n","model.add(MaxPooling1D(pool_size=2))\n","model.add(LSTM(100))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","```\n","Let's try out this model.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZHhiR1Y3gFJj","executionInfo":{"status":"ok","timestamp":1616495992844,"user_tz":-480,"elapsed":2740632,"user":{"displayName":"Jimmy Goh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLSkpAlGxDPhWOsoJbyGqn037GeHQO859TaFZZ=s64","userId":"14200143294426892711"}},"outputId":"81b79244-1685-4e4d-ac1e-19a5bd8a3726"},"source":["from keras.layers import Conv1D\n","from keras.layers import MaxPooling1D\n","\n","# create the model\n","model = Sequential()\n","model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n","model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')) \n","model.add(MaxPooling1D(pool_size=2))\n","model.add(LSTM(100))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_3 (Embedding)      (None, 500, 32)           160000    \n","_________________________________________________________________\n","conv1d (Conv1D)              (None, 500, 32)           3104      \n","_________________________________________________________________\n","max_pooling1d (MaxPooling1D) (None, 250, 32)           0         \n","_________________________________________________________________\n","lstm_3 (LSTM)                (None, 100)               53200     \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 1)                 101       \n","=================================================================\n","Total params: 216,405\n","Trainable params: 216,405\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1wQrS5kupIjI","executionInfo":{"status":"ok","timestamp":1616496114782,"user_tz":-480,"elapsed":38944,"user":{"displayName":"Jimmy Goh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLSkpAlGxDPhWOsoJbyGqn037GeHQO859TaFZZ=s64","userId":"14200143294426892711"}},"outputId":"2faee8bf-9bfe-4b30-c414-8b3ba056ee5e"},"source":["%%time\n","model.fit(X_train, y_train, epochs=3, batch_size=64)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Epoch 1/3\n","391/391 [==============================] - 14s 30ms/step - loss: 0.6169 - accuracy: 0.6267\n","Epoch 2/3\n","391/391 [==============================] - 12s 30ms/step - loss: 0.2615 - accuracy: 0.8935\n","Epoch 3/3\n","391/391 [==============================] - 12s 31ms/step - loss: 0.1921 - accuracy: 0.9300\n","CPU times: user 39.1 s, sys: 1.86 s, total: 41 s\n","Wall time: 38.2 s\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f04ade14890>"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"honLTilCjgJw","executionInfo":{"status":"ok","timestamp":1616496022281,"user_tz":-480,"elapsed":2770049,"user":{"displayName":"Jimmy Goh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLSkpAlGxDPhWOsoJbyGqn037GeHQO859TaFZZ=s64","userId":"14200143294426892711"}},"outputId":"ea9d0c3a-d43c-45d6-c568-d93dbecda644"},"source":["# Final evaluation of the model\n","scores = model.evaluate(X_test, y_test, verbose=0)\n","print(\"Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Accuracy: 88.38%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1r0LI7c0jlnD"},"source":["We can see that we achieve similar results to the first example although with less weights and faster training time. We can expect that even better results could be achieved if this example was further extended to use dropout."]},{"cell_type":"markdown","metadata":{"id":"EKZ1GBQU10pT"},"source":["### Exercise 1\n","\n","Adjust the appropriate model to achieve an accuracy of above 90%."]},{"cell_type":"markdown","metadata":{"id":"HvC65-pF0730"},"source":["### Exercise 2\n","\n","Change the dataset to Keras's **Reuters newswire classification dataset**.<br>\n","Click [here](https://keras.io/api/datasets/reuters/) to find out more on the reuters dataset."]},{"cell_type":"code","metadata":{"id":"bFOJtX4w1Fyr","executionInfo":{"status":"ok","timestamp":1616496022282,"user_tz":-480,"elapsed":2770048,"user":{"displayName":"Jimmy Goh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLSkpAlGxDPhWOsoJbyGqn037GeHQO859TaFZZ=s64","userId":"14200143294426892711"}}},"source":[""],"execution_count":20,"outputs":[]}]}